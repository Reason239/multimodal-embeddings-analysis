{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkpcW--1XQb9"
      },
      "source": [
        "# Precomputing the embeddings\n",
        "\n",
        "[COCO 2015 Image Captioning Task](https://cocodataset.org/#captions-2015)\n",
        "\n",
        "[coco-caption/annotations/](https://github.com/tylin/coco-caption/tree/master/annotations)\n",
        "\n",
        "[CLIP Colab Notebook](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb)\n",
        "\n",
        "[Reference for working with the COCO Captions dataset](https://github.com/vidyasrimani/ComputerVision_ImageCaptioning)\n",
        "\n",
        "[CIFAR100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5zAZ-PRZCLf"
      },
      "source": [
        "Prepare the COCO Captions dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCKmNxhiXLHQ",
        "outputId": "614a0d20-b1cd-44ca-d458-e8a712ef15ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-26 14:15:41--  http://images.cocodataset.org/zips/val2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.198.33\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.198.33|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6645013297 (6.2G) [application/zip]\n",
            "Saving to: ‘val2014.zip’\n",
            "\n",
            "val2014.zip         100%[===================>]   6.19G  43.8MB/s    in 2m 22s  \n",
            "\n",
            "2022-05-26 14:18:03 (44.7 MB/s) - ‘val2014.zip’ saved [6645013297/6645013297]\n",
            "\n",
            "--2022-05-26 14:18:03--  https://raw.githubusercontent.com/tylin/coco-caption/master/annotations/captions_val2014.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29707814 (28M) [text/plain]\n",
            "Saving to: ‘captions_val2014.json’\n",
            "\n",
            "captions_val2014.js 100%[===================>]  28.33M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-05-26 14:18:05 (248 MB/s) - ‘captions_val2014.json’ saved [29707814/29707814]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget http://images.cocodataset.org/zips/val2014.zip\n",
        "! wget https://raw.githubusercontent.com/tylin/coco-caption/master/annotations/captions_val2014.json\n",
        "! unzip -q val2014.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbqTagARXsWQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.datasets import CocoDetection, CIFAR100\n",
        "import torchvision.transforms as transforms\n",
        "import pickle as pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvQi-VOkrMV1"
      },
      "outputs": [],
      "source": [
        "class UniformCocoCaptions(CocoDetection):\n",
        "    \"\"\"`Same as torchvision.datasets.CocoCaptions, but \n",
        "    always outputs exactly 5 captions (for batch formation)\n",
        "    \"\"\"\n",
        "\n",
        "    def _load_target(self, id: int):\n",
        "        return [ann[\"caption\"] for ann in super()._load_target(id)[:5]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare CIFAR-100 dataset"
      ],
      "metadata": {
        "id": "CA7nz3Z_Px9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR100(root='datasets/cifar100', train=True, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "e766a6120324473d93c0418ab67bf448",
            "8ef2d4ac5e4145e295c7bd72f7a75eed",
            "522121a7efb84a02a0b5aa00f5a3c0e0",
            "31c0184c5e7f418bb68cf339fdc7bb43",
            "15bd3deab47746fdb98e30a823ee400d",
            "449af143c1be49b7a68e1aca146b973b",
            "39f2ac04e8154dc3b75e9393591773d9",
            "b4e2795516a246c2a6e4825bd11096a6",
            "48f0167f65e54178a3139f3e01978da2",
            "6e25f657babb4dfdb2848cc19a9638ba",
            "a7b642cc6b6b45598a11cae4712890ad"
          ]
        },
        "id": "WP5RdugFP4c_",
        "outputId": "df2807b2-133b-4d09-ceaa-6aab0877f399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to datasets/cifar100/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e766a6120324473d93c0418ab67bf448"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/cifar100/cifar-100-python.tar.gz to datasets/cifar100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR100\n",
              "    Number of datapoints: 50000\n",
              "    Root location: datasets/cifar100\n",
              "    Split: Train"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('datasets/cifar100/cifar-100-python/meta', 'rb') as f:\n",
        "    class_names = pkl.load(f)['fine_label_names']\n",
        "assert len(class_names) == 100\n",
        "class_names[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRKEMgK6QnW5",
        "outputId": "6da26187-da95-42e6-d138-7c3fdbe7b2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apple', 'aquarium_fish', 'baby']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = CIFAR100(root='datasets/cifar100', train=True, download=False)[0]"
      ],
      "metadata": {
        "id": "P1-zUdbxRV53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "A0NhJTpBRfrN",
        "outputId": "6bb8e02e-70cb-461c-f5a9-fe489f888340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FBCCC63A290>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names[label]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BJC6cPylRlpa",
        "outputId": "92873f74-71ed-45c1-908f-8d284ac1282e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cattle'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn_I5_58ZFDd"
      },
      "source": [
        "Prepare the CLIP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOmn38fEZBjV",
        "outputId": "98ca50c3-52f9-42be-b274-c31e5b97cf0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-f9mqrnt5\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-f9mqrnt5\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.12.0+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369387 sha256=78eaaee1cd5f41487ed6f1e53956d12e1321efd15b17b71b2e9595ca9933916d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s_zoyqxf/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi8rUSKCZMMU",
        "outputId": "60909d9c-478b-4b31-b77c-767634dcea6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfqSW7RfjsXc"
      },
      "source": [
        "Compute the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fplp_6YgZ5-Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTu2vDLXZ3gD"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_clip_coco_embeds(clip_model_name, coco_images_dir, coco_captions_file,\n",
        "                             batch_size=64, save_root=None,\n",
        "                             image_embeds_filename_prefix=None,\n",
        "                             text_embeds_filename_prefix=None,\n",
        "                             save_all_texts=False):\n",
        "    if not image_embeds_filename_prefix:\n",
        "        image_embeds_filename_prefix = ''\n",
        "    else:\n",
        "        image_embeds_filename_prefix += '_'\n",
        "    if not text_embeds_filename_prefix:\n",
        "        text_embeds_filename_prefix = ''\n",
        "    else:\n",
        "        text_embeds_filename_prefix += '_'\n",
        "    image_save_dir = Path(save_root) / 'image'\n",
        "    image_save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    text_save_dir = Path(save_root) / 'text'\n",
        "    text_save_dir.mkdir(parents=True, exist_ok=True)    \n",
        "\n",
        "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(f'Using {device}')\n",
        "    print('Loading CLIP')\n",
        "    model, preprocess = clip.load(clip_model_name)\n",
        "    model.to(device)\n",
        "    print('Done')\n",
        "    dataset = UniformCocoCaptions(root='/content/val2014',\n",
        "                    annFile='captions_val2014.json',\n",
        "                    transform=preprocess)\n",
        "    print('COCO dataset:\\n', dataset)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                            pin_memory=torch.cuda.is_available(), num_workers=2)\n",
        "\n",
        "    all_image_embeddings = []\n",
        "    all_text_embeddings = []\n",
        "    print('Computing embeddings')\n",
        "    for batch in tqdm(dataloader):\n",
        "        images, texts = batch\n",
        "        image_input = images.to(device)\n",
        "        # texsts have 5 tuples with batch_size texts\n",
        "        # rearrange them, so that the 5 texts are next to each other\n",
        "        texts = list(chain(*list(zip(*texts))))\n",
        "        text_tokens = clip.tokenize(texts).to(device)\n",
        "        image_features = model.encode_image(image_input).float()\n",
        "        text_features = model.encode_text(text_tokens).float()\n",
        "        all_image_embeddings.append(image_features)\n",
        "        all_text_embeddings.append(text_features)\n",
        "    embed_dim = all_image_embeddings.shape[-1]\n",
        "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
        "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0).reshape(-1, 5, embed_dim)\n",
        "\n",
        "    image_embeds_filename = image_embeds_filename_prefix + \\\n",
        "                            f'CLIP_{clip_model_name.replace(\"/\", \"-\")}.pt'\n",
        "    torch.save(all_image_embeddings, image_save_dir / image_embeds_filename)\n",
        "    for i in range(5 if save_all_texts else 1):\n",
        "        text_embeds_filename = text_embeds_filename_prefix + \\\n",
        "                               f'CLIP_{clip_model_name.replace(\"/\", \"-\")}_{i}.pt'\n",
        "        torch.save(torch.clone(all_text_embeddings[:, i, :]), text_save_dir / text_embeds_filename)\n",
        "    print('Done')\n",
        "    print(f'Image embeddings: {all_image_embeddings.shape}')\n",
        "    print(f'Text  embeddings: {all_text_embeddings.shape}')\n",
        "    gc.collect()\n",
        "\n",
        "@torch.no_grad()\n",
        "def compute_clip_cifar_embeds(clip_model_name, dataset_root, train,\n",
        "                              batch_size=256, save_root=None,\n",
        "                              image_embeds_filename_prefix=None,\n",
        "                              label_filename_prefix=None,\n",
        "                              text_embeds_filename_prefix=None,\n",
        "                              template='low-resolution photo of a {}'):\n",
        "    if not image_embeds_filename_prefix:\n",
        "        image_embeds_filename_prefix = ''\n",
        "    else:\n",
        "        image_embeds_filename_prefix += '_'\n",
        "    if not label_filename_prefix:\n",
        "        label_filename_prefix = ''\n",
        "    else:\n",
        "        label_filename_prefix += '_'\n",
        "    if not text_embeds_filename_prefix:\n",
        "        text_embeds_filename_prefix = ''\n",
        "    else:\n",
        "        text_embeds_filename_prefix += '_'\n",
        "    image_save_dir = Path(save_root) / 'image'\n",
        "    image_save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    label_save_dir = Path(save_root) / 'label'\n",
        "    label_save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    text_save_dir = Path(save_root) / 'text'\n",
        "    text_save_dir.mkdir(parents=True, exist_ok=True)    \n",
        "\n",
        "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    print(f'Using {device}')\n",
        "    print('Loading CLIP')\n",
        "    model, preprocess = clip.load(clip_model_name)\n",
        "    model.to(device)\n",
        "    print('Done')\n",
        "    dataset = CIFAR100(dataset_root, train, transform=preprocess, download=False)\n",
        "    print('CIFAR100 dataset:\\n', dataset)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                            pin_memory=torch.cuda.is_available())#, num_workers=2)\n",
        "\n",
        "    all_image_embeddings = []\n",
        "    all_labels = []\n",
        "    print('Computing embeddings')\n",
        "    for batch in tqdm(dataloader):\n",
        "        images, labels = batch\n",
        "        image_input = images.to(device)\n",
        "        image_features = model.encode_image(image_input).float()\n",
        "        all_image_embeddings.append(image_features)\n",
        "        all_labels.append(labels)\n",
        "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    image_embeds_filename = image_embeds_filename_prefix + \\\n",
        "                            f'CLIP_{clip_model_name.replace(\"/\", \"-\")}.pt'\n",
        "    label_filename = label_filename_prefix + \\\n",
        "                            f'CLIP_{clip_model_name.replace(\"/\", \"-\")}.pt'\n",
        "    torch.save(all_image_embeddings, image_save_dir / image_embeds_filename)\n",
        "    torch.save(all_labels, label_save_dir / label_filename)\n",
        "    with open('datasets/cifar100/cifar-100-python/meta', 'rb') as f:\n",
        "        class_names = pkl.load(f)['fine_label_names']\n",
        "    texts = [template.format(name) for name in class_names]\n",
        "    text_tokens = clip.tokenize(texts).to(device)\n",
        "    text_features = model.encode_text(text_tokens).float()\n",
        "    text_embeds_filename = text_embeds_filename_prefix + \\\n",
        "                            f'CLIP_{clip_model_name.replace(\"/\", \"-\")}.pt'\n",
        "    torch.save(text_features, text_save_dir / text_embeds_filename)\n",
        "    print('Done')\n",
        "    print(f'Image embeddings: {all_image_embeddings.shape}')\n",
        "    print(f'Labels: {all_labels.shape}')\n",
        "    print(f'Text ({template.format(\"<class>\")}) embeddings: {text_features.shape}')\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clip_models = [#'RN50',\n",
        "               'RN101',\n",
        "               #'RN50x4',\n",
        "               #'RN50x16',\n",
        "               #'RN50x64',\n",
        "               'ViT-B/32',\n",
        "               #'ViT-B/16',\n",
        "               #'ViT-L/14',\n",
        "               #'ViT-L/14@336px']\n",
        "              ]            "
      ],
      "metadata": {
        "id": "khw62j3vXqfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for clip_model_name in clip_models:\n",
        "    print(clip_model_name)\n",
        "    clip.load(clip_model_name)"
      ],
      "metadata": {
        "id": "NQhqBRo0cdcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsxvAf4XaHWM"
      },
      "outputs": [],
      "source": [
        "clip_model_name = 'RN50'\n",
        "coco_images_dir = 'val2014'\n",
        "coco_captions_file = 'captions_val2014.json'\n",
        "save_root = 'embeddings/coco_val2014'\n",
        "image_embeds_filename_prefix = None\n",
        "text_embeds_filename_prefix = None\n",
        "batch_size = 256\n",
        "# for clip_model_name in clip_models:\n",
        "compute_clip_coco_embeds(clip_model_name, coco_images_dir, coco_captions_file,\n",
        "                            batch_size, save_root,\n",
        "                            image_embeds_filename_prefix,\n",
        "                            text_embeds_filename_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clip_model_name = 'ViT-B/32'\n",
        "dataset_root = 'datasets/cifar100'\n",
        "train = True\n",
        "save_root = f'embeddings/CIFAR100{\"train\" if train else \"test\"}'\n",
        "template = 'low-resolution photo of a {}'\n",
        "image_embeds_filename_prefix = None\n",
        "label_filename_prefix = None\n",
        "text_embeds_filename_prefix = None\n",
        "batch_size = 256\n",
        "for clip_model_name in clip_models:\n",
        "    compute_clip_cifar_embeds(clip_model_name, dataset_root, train,\n",
        "                              batch_size, save_root,\n",
        "                              image_embeds_filename_prefix,\n",
        "                              label_filename_prefix,\n",
        "                              text_embeds_filename_prefix)"
      ],
      "metadata": {
        "id": "GnUXCiSIWRT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clip_model_name = 'ViT-B/32'\n",
        "train = False\n",
        "save_root = f'embeddings/CIFAR100{\"train\" if train else \"test\"}'\n",
        "for clip_model_name in clip_models:\n",
        "    compute_clip_cifar_embeds(clip_model_name, dataset_root, train,\n",
        "                              batch_size, save_root,\n",
        "                              image_embeds_filename_prefix,\n",
        "                              label_filename_prefix,\n",
        "                              text_embeds_filename_prefix)"
      ],
      "metadata": {
        "id": "_8o9zNAdYsTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! du -sh /content/embeddings"
      ],
      "metadata": {
        "id": "sxUVuqojhvuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAK5DNZtg96t",
        "outputId": "7f952864-0d12-4d9d-fb09-4a7a3cf976ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -R /content/embeddings /content/drive/MyDrive/DL_NLP_project/embeddings"
      ],
      "metadata": {
        "id": "9p5Rx640hAq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! zip -r embeddings.zip embeddings/"
      ],
      "metadata": {
        "id": "H2hJpgK8YkK0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "precompute_coco_embeddings.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e766a6120324473d93c0418ab67bf448": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ef2d4ac5e4145e295c7bd72f7a75eed",
              "IPY_MODEL_522121a7efb84a02a0b5aa00f5a3c0e0",
              "IPY_MODEL_31c0184c5e7f418bb68cf339fdc7bb43"
            ],
            "layout": "IPY_MODEL_15bd3deab47746fdb98e30a823ee400d"
          }
        },
        "8ef2d4ac5e4145e295c7bd72f7a75eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_449af143c1be49b7a68e1aca146b973b",
            "placeholder": "​",
            "style": "IPY_MODEL_39f2ac04e8154dc3b75e9393591773d9",
            "value": ""
          }
        },
        "522121a7efb84a02a0b5aa00f5a3c0e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4e2795516a246c2a6e4825bd11096a6",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48f0167f65e54178a3139f3e01978da2",
            "value": 169001437
          }
        },
        "31c0184c5e7f418bb68cf339fdc7bb43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e25f657babb4dfdb2848cc19a9638ba",
            "placeholder": "​",
            "style": "IPY_MODEL_a7b642cc6b6b45598a11cae4712890ad",
            "value": " 169001984/? [00:12&lt;00:00, 13649976.15it/s]"
          }
        },
        "15bd3deab47746fdb98e30a823ee400d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "449af143c1be49b7a68e1aca146b973b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f2ac04e8154dc3b75e9393591773d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4e2795516a246c2a6e4825bd11096a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f0167f65e54178a3139f3e01978da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e25f657babb4dfdb2848cc19a9638ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b642cc6b6b45598a11cae4712890ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}